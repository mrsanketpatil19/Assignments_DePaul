---
title: "DSC-424 Midterm"
author: "Sanket Patil"
date: "2024-02-14"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# -------------------------------------------------------------------------------------------

# Question 1:
1) How are they applying Factor Analysis?
--> In this study, the researchers applied exploratory factor analysis (EFA) to examine the underlying structure of technostress among primary school teachers. EFA is a statistical technique used to identify the underlying structure of a set of variables and to group them into meaningful dimensions or factors.
Steps:
  i) Item Selection: The researchers started by selecting 28 items related to technostress from previous literature.
  ii) Expert Verification: The modified and translated items were sent to experts for verification of content validity, face validity, and criterion validity. 
  iii) Pilot Study: A pilot study was conducted with 106 primary school teachers to collect data using the newly developed questionnaire.
  iv) Exploratory Factor Analysis (EFA): 
  a) Data Preparation: The researchers looked at the data collected from the pilot study. They used a method called principal component analysis to explore the data and find patterns.
  b) KMO and Bartlett's Test: They checked if the data was good enough for their analysis by doing two tests. The tests told them that the data was good and could be used for their analysis.
  c) Factor Extraction: They tried to find the main factors or groups in the data that explained most of the differences between the responses. They kept factors that were really important, based on a certain value.
  d) Factor Rotation: They adjusted the factors they found to make them easier to understand. This helped them see clearer patterns in the data.
  e) Factor Interpretation: They looked at each item in the questionnaire to see which factor it belonged to. They kept items that fit well with a factor and made sense.
  f) Dimensionality Determination: They found five main groups or dimensions in the data that explained technostress among primary school teachers.
  g) Reliability Analysis: They checked if the items they kept in the questionnaire were consistent and reliable. If items were consistent, they were more confident that their questionnaire accurately measured technostress.
The researchers used factor analysis to figure out the different aspects of technostress experienced by primary school teachers. They found five main dimensions of technostress and created a dependable tool to measure it.
  
2) What kind of rotation do they use?
--> The researchers used Varimax rotation in the Exploratory Factor Analysis (EFA) procedure. Varimax rotation is a popular orthogonal rotation method that aims to maximize the variance of the squared loadings on each factor, making it easier to interpret the factors. In the context of factor analysis, rotation helps simplify the pattern of loadings and makes it easier to understand the relationships between variables and factors. The Varimax rotation method is commonly used when the factors are expected to be uncorrelated, which is a common assumption in many factor analysis applications. 

3) How many components do they concentrate on in their analysis? How did they arrive at these number of components?
--> In their analysis, the researchers focused on five components. They arrived at this number of components through an Exploratory Factor Analysis (EFA) procedure, specifically employing Principal Component Analysis (PCA) with Varimax rotation.

4) Explain the breakdown of the components and the significance of their names.
--> Technical Oriented: This means teachers feeling stressed about using technology because it's tricky. They might struggle with computer programs, fixing broken equipment, or learning new tech stuff.
Profession Oriented: This is about teachers feeling stressed because they worry about how well they're using technology in their job. They might feel pressure to be really good with tech, keep up with new teaching tools, or make sure they're using tech in the best way for teaching.
Personal Oriented: This is when teachers feel stressed because technology makes their personal life busy or overwhelming. They might feel tired or frustrated from always being connected or having too much to do because of technology.
Social Oriented: This is about stress from how technology affects relationships and social life. Teachers might worry about balancing work and personal time because of technology, or they might feel pressure from others to use technology in certain ways.
Teaching-Learning Process Oriented: This means stress from using technology to teach and learn. Teachers might find it hard to adapt lessons for online learning, deal with students' different tech skills, or manage distractions caused by technology in class.
The names of these components are important because they help us understand different ways teachers feel stressed about using technology. By breaking down technostress into these categories, the study helps us see that it's not just one type of stress, but many. This gives us a better idea of how technology affects teachers' feelings and work. Also, these categories help researchers organize the information they collected in the study. They can use these categories to see which areas of technostress are the most concerning for teachers. This can help them come up with ideas to help teachers deal with stress from technology better.

5) How do they evaluate the stability of the components (i.e. factorability)?
--> To evaluate the stability of the components or factorability, the researchers employed varous statistical methods and criteria:
Kaiser-Meyer-Olkin (KMO) Measure of Sampling Adequacy: 
The KMO measure checks if our data is good for studying patterns. It tells us if the connections between different things we're studying are strong enough. If the KMO value is above 0.6, it means our data is probably good for studying these patterns. In our study, the KMO value was 0.884, so our data looked good.

Bartlett's Test of Sphericity:
Bartlett's test checks if the connections between different things we're studying are strong enough for us to find meaningful patterns. If the result of Bartlett's test is significant (usually with a p-value less than 0.05), it means our data is suitable for finding these patterns. In our study, Bartlett's test showed a significant result with a p-value less than 0.001, indicating our data was good for finding patterns.

Eigenvalues:
Eigenvalues tell us how much each pattern we find explains the differences in our data. If an eigenvalue is above 1.0, it means that pattern is important and explains a lot of the differences. In our study, we looked at eigenvalues for each pattern we found, and those above 1.0 were considered important.

Factor Loadings:
Factor loadings show how much each thing we're studying relates to the patterns we found. Higher factor loadings mean stronger connections. We looked at factor loadings to see which things were most strongly connected to each pattern. Items with high factor loadings, usually above 0.55, were seen as important for that pattern.

By checking these things, the researchers made sure the patterns they found were reliable. The high KMO value, significant Bartlett's test, eigenvalues above 1.0, and strong factor loadings showed that the patterns they found were trustworthy representations of what they were studying, which in this case was technostress.

6) Do they use these components in later analysis, such as regression?  If so, what do they discovery?
--> The researchers not used these components directly in this study. But they might use these components identified through exploratory factor analysis (EFA) in further analysis such as regression.
 In regression analysis, these components can serve as independent variables to predict or explain variability in a dependent variable.
Based on the components identified in the study:
Technical oriented :  In regression analysis, they may find that higher levels of stress in this dimension are associated with higher overall technostress levels among teachers.
Profession oriented : Regression analysis might reveal that stress in this dimension significantly predicts technostress, indicating that the professional context plays a role in teachers' experiences of technostress.
Personal oriented : Regression analysis could show that personal factors significantly contribute to technostress levels, highlighting the importance of considering individual differences in understanding and addressing technostress.
Social oriented : Regression analysis may demonstrate that social factors play a significant role in shaping technostress experiences among teachers.
Teaching-learning process oriented : Regression analysis might reveal that challenges or difficulties in adapting teaching methods to technology contribute to overall technostress levels among teachers.

The researchers would likely investigate how each of these dimensions contributes to technostress among primary school teachers. They might conduct regression analyses to examine the relationship between these dimensions and technostress, controlling for relevant covariates.

By looking at how these different aspects, like technical problems or pressures from work, relate to technostress in teachers, researchers can figure out what exactly makes teachers stressed about using technology. This helps them find ways to help teachers deal with this stress better in schools.

# 7) What overall conclusions does Principal Component Analysis allow them to draw?
--> PCA helps researchers understand technostress among primary school teachers better by breaking it down into different parts:

Different Aspects: Technostress is not just one thing; it's made up of five main parts: technology-related stress, stress from professional demands, stress from personal experiences, stress from social interactions, and stress related to teaching and learning.

Understanding the Factors: By using PCA, researchers can figure out how much each of these parts contributes to overall technostress. This helps them see which parts are most important and how they all fit together.

Finding Important Items: PCA also helps researchers figure out which specific questions or statements are most important for measuring technostress. Some questions might be really good at showing technostress, while others might not be as helpful.

Checking if it's Reliable: Researchers also use PCA to check if their questions are consistent and stable. If they are, it means the questions are good at measuring technostress in different situations.

Making Sure it Works: Finally, PCA helps researchers make sure that the questions they're asking really do measure technostress among primary school teachers. If the analysis shows that the questions are valid and reliable, then they can trust the results they get from using them.

Overall, PCA helps researchers understand technostress better so they can find ways to help teachers deal with it and feel better at work.

'
# ---------------------------------------------------------------------------------------------------------------

# Question 2 

# Define the matrices and vectors
Z <- matrix(c(1, 1, 1, 1, 9,5,-3,11), nrow=4, byrow=FALSE)
Z

Y <-  matrix(c(-1, 6, 0, 8), nrow=4, byrow=FALSE)
Y

M <- matrix(c(1, 11, 0,
              42, 52, 35,
              0, 9, 3), nrow=3, byrow=TRUE)
M

N <- matrix(c(-10,-10,0,
              0,10,20,
              10,20,10), nrow=3, byrow=TRUE)
N

v <- matrix(c(-11, 11, 22), nrow=3)
v

w <- matrix(c(8,-2,4), nrow=3)           
w


# v.w (dot product)

v_dot_w <- sum(v * w)
v_dot_w

# Scalar multiplication of -3 with w
neg_3_w <- -3 * w
neg_3_w

# Matrix-vector multiplication of M and v
M_times_v <- M %*% v
M_times_v

# Matrix addition of M and N
M_plus_N <- M + N
M_plus_N


# Matrix subtraction of M and N
M_minus_N <- M - N
M_minus_N

# Z transpose times Z
Z_transpose_times_Z <- crossprod(Z)
Z_transpose_times_Z

# Inverse of Z_transpose_times_Z
Z_transpose_times_Z_inv <- solve(Z_transpose_times_Z)
Z_transpose_times_Z_inv

# Z transpose times Y
Z_transpose_times_Y <- t(Z) %*% Y
Z_transpose_times_Y


# Calculate B
B <- Z_transpose_times_Z_inv %*% Z_transpose_times_Y
B

# Determinant of Z_transpose_times_Z
det_Z_transpose_times_Z <- det(Z_transpose_times_Z)
det_Z_transpose_times_Z




# -------------------------------------------------------------------------------------------

# Question No. 3: 
What are the different ways of treating missing values? Give examples that show the benefits or disadvantages of using these different strategies.

--> 
Treating missing values is a crucial step in data preprocessing, and there are several strategies to handle them. Each strategy has its own benefits and disadvantages, and the choice depends on the specific characteristics of the dataset and the goals of the analysis. 

Below are some of the ways of treating missing values:
  
  1) Deletion: Rows or columns containing missing values are entirely removed from the dataset.
Advantages: Simple and straightforward
Disadvantages: It can lead to loss of valuable information, especially if the missing values are not randomly distributed. This approach may result in biased analysis if the missing data is related to the outcome of interest.
Example: Suppose we have a dataset of customer reviews for a product, and one of the columns is "Age" where some rows have missing values. If we delete rows with missing age values, we will lose valuable information about customers' age demographics, which could be important for targeted marketing campaigns.

2) Imputation: We can replace missing values with the help of mean, median, mode, or we can also predict the missing values with the help of algorithm such as KNN.
Advantages: Retains all observations in the dataset, prevents information loss, and maintains sample size.
Disadvantages: Imputed values may introduce bias or distort the original distribution of the variable.
Example: If we have numeric variables, we can replace missing values with mean or median and if we have categorical variable, we can replace missing values by mode.

3) Prediction Models: We can predict the missing values using machine learning algorithms with the help of other features in the dataset.
Advantages: Utilizes relationships between variables to make more accurate predictions. Can handle complex patterns in missing data.
Disadvantages: Requires computational resources and may overfit the data. Not suitable for large datasets with high NA values.
Example: If temperature readings are missing for certain days, a machine learning model trained on other weather variables like humidity, pressure, and wind speed can predict the missing temperature values.

4) Flagging and Encoding: We can create an additional binary indicator variable to signify if the value was missing or not.
Advantages: Preserves the information that a value was missing, allowing models to account for the missingness pattern. Can be combined with imputation methods.
Disadvantages: Increases the dimensionality of the dataset and may introduce noise if the missingness pattern is not informative.

5) Domain Knowledge: Use domain knowledge or expert judgment to fill in missing values based on context.
Advantages: It incorporates subject matter expertise into the imputation process, leading to more meaningful results.
Disadvantages: Subjective and may introduce bias if the expert judgment is incorrect or inconsistent. Also it is time consuming as it will require manual efforts.
Example: In a healthcare dataset, if a patient's weight is missing, a medical professional may use their knowledge of the patient's medical history, demographics, and health condition to estimate a reasonable weight value.


# -----------------------------------------------------------------------------------------------

# Question 4:
Explain how to use R to check for the four assumptions of linear regression.

--> 

To check for the four assumptions of linear regression in R, we can follow below steps:

1) Linearity between variables:
Check for linearity between the independent variables and the dependent variable.
library(ggplot2)
Let's assume df is our dataframe and y is our dependent variable with X1 and X2 as independent variables.
# Plot each independent variable against the dependent variable
ggplot(df, aes(x = x1, y = y)) + geom_point() + geom_smooth(method = "lm", se = FALSE)
ggplot(df, aes(x = x2, y = y)) + geom_point() + geom_smooth(method = "lm", se = FALSE)

2) There should be less or no multicolinearity:
  To check correlation, we can use below function in R.
cor(df)
With the help of corrplot library, we can visualize the correlation matrix.
corrplot(correlation_matrix, method = "circle")

If the correlation value is greater than 0.7 or 0.8, we can say that variable is having high correlation value.

Check the Variance Inflation Factor (VIF): 
  Calculate VIF for each independent variable to assess multicollinearity.
library(car)
vif_values <- vif(lm(y ~ ., data = df)) # y is dependent variable and df is dataframe
print(vif_values)

If the VIF value is greater than 10, we can say that there is multicollinearity present. We can remove those variables.

3) Error should be normally distributed:
  # Extract residuals
  residuals <- resid(model)
  
  # Plot histogram of residuals
  hist(residuals, main = "Histogram of Residuals", xlab = "Residuals")
  skewness(residuals)
  
  # Q-Q plot of residuals
  qqnorm(residuals)
  qqline(residuals)
  
  Resiuduals should follow a normal destribution with ideal skewness value of 0. We can check this with the help of above sample code in R.
  
  4) Homoscedasticity: Check if residuals have constant variance across different levels of the independent variables.
# Extract residuals
residuals <- residuals(model)    # Model is trained regression model

# Extract fitted values
fitted_values <- fitted(model)

# Create a data frame for plotting
plot_data <- data.frame(Fitted = fitted_values, Residuals = residuals)

# Plot residuals against fitted values
ggplot(plot_data, aes(x = Fitted, y = Residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs Fitted Values Plot",
       x = "Fitted Values",
       y = "Residuals")

If the spread of residuals is roughly constant across all levels of fitted values, homoscedasticity is met.
If the spread of residuals varies systematically across different levels of fitted values, heteroscedasticity may be present.
The residuals should exhibit a random pattern around zero when plotted against the predicted values.

# --------------------------------------------------------------------------------------------------

# Question 5: 

What are the advantages and disadvantages of using ridge and lasso regressions? Give examples of when you would use ridge compared to when you would use lasso regression.

-->
  
  Ridge Regression and Lasso Regression are both regularization techniques used in linear regression. These techniques are used to address the multicollinearity and prevent overfitting.

Ridge Regression: 
  Ridge regression is a type of linear regression that adds a penalty term to the ordinary least squares (OLS) method, which helps to shrink the coefficients towards zero. This penalty term is proportional to the square of the coefficients, hence we call it "ridge" or L2, and it's controlled by a parameter called lambda (Î»).

Advantages: 
Ridge regression helps to mitigate multicollinearity, which occurs when independent variables are highly correlated with each other. 
It works well even when the number of predictors is greater than the number of observations. This will prevent overfitting.

Disadvantages:
Ridge regression does not perform variable selection, hence it keeps all predictors in the model regardless of their importance. This can make the model less interpretable.
It may not be suitable for scenarios where identifying the most influential predictors is essential.

Example:
Suppose we are building a model to predict housing prices based on various features like square footage, number of bedrooms, and distance to amenities. If some of these features are highly correlated, ridge regression can effectively handle this correlation and produce more reliable predictions.


Lasso Regression:

Explanation: Lasso regression, short for Least Absolute Shrinkage and Selection Operator, is another form of linear regression that adds a penalty term to the OLS method. However, unlike ridge regression, lasso uses the absolute values of the coefficients as the penalty term. Hence it is also called as L1.

Advantages:
Lasso regression performs both parameter shrinkage and variable selection, making it useful for models with a large number of predictors. It tends to shrink less important coefficients to zero, which will effectively eliminate them from the model.
It can generate more interpretable models by automatically selecting the most relevant predictors.

Disadvantages:
Lasso regression can be sensitive to outliers in the data, potentially leading to biased coefficient estimates.

Example:
Consider a scenario where we are analyzing customer data to predict their likelihood of purchasing a product. We have numerous customer attributes such as age, income, and purchase history. Lasso regression can help identify the most influential factors in predicting purchase behavior while disregarding less relevant variables, resulting in a more concise and interpretable model.

# ----------------------------------------------------------------------------------------------------------------- 

# Question 6:
The researcher should use an orthogonal rotation matrix to properly interpret the factors. Orthogonal rotation methods, such as Varimax or Quartimax, ensure that the resulting factors are uncorrelated with each other, making the interpretation of each factor more straightforward.

--> 

When factors are correlated, it can be challenging to understand the unique contribution of each factor to the underlying constructs being measured. Orthogonal rotation helps in simplifying the factor structure by maximizing the variance of factor loadings within each factor while minimizing the variance of factor loadings across factors, thus enhancing the interpretability of the factors.

In contrast, oblique rotation methods allow for factors to be correlated with each other, which can sometimes be more realistic depending on the underlying theoretical framework. However, in cases where factors are meant to be independent or when simpler interpretation is desired, orthogonal rotation is typically preferred.

The researcher should use an orthogonal rotation matrix to properly interpret the factors. Orthogonal rotation methods, such as Varimax or Quartimax, ensure that the resulting factors are uncorrelated with each other, making the interpretation of each factor more straightforward.

When factors are correlated, it can be challenging to understand the unique contribution of each factor to the underlying constructs being measured. Orthogonal rotation helps in simplifying the factor structure by maximizing the variance of factor loadings within each factor while minimizing the variance of factor loadings across factors, thus enhancing the interpretability of the factors.

In contrast, oblique rotation methods allow for factors to be correlated with each other, which can sometimes be more realistic depending on the underlying theoretical framework. However, in cases where factors are meant to be independent or when simpler interpretation is desired, orthogonal rotation is typically preferred.

# -----------------------------------------------------------------------------------------------------

# Question 7:
What are the advantages and disadvantages of using exploratory factor analysis versus principal component analysis?
-->

Exploratory Factor Analysis (EFA):

Advantages:
Helps to understand the underlying structure or patterns in your data.
Identifies latent (hidden) variables that may not be directly observed.
Provides insight into relationships between variables.
Allows for the testing of theoretical models.

Disadvantages:
Requires a larger sample size for accurate results.
More complex interpretation compared to principal component analysis.
Assumes that variables are normally distributed.
Results can be sensitive to different extraction methods and rotation techniques.


Principal Component Analysis (PCA):

Advantages:
Reduces the curse of dimensionality.
Simplifies data by reducing dimensionality while retaining most of the variation.
Easy to understand and interpret.
Less stringent assumptions compared to EFA.
Useful for data compression and visualization.

Disadvantages:
May not always capture underlying factors if correlations between variables are weak.
Does not differentiate between common and unique variance.
Assumes linear relationships between variables.
May not be suitable for identifying latent variables.

# -----------------------------------------------------------------------------------------------------

#Question 8:
You are conducting a study to predict what a student's grade will be in a class using linear regression.  How would you analyze this study?  What would you write in your statistical analysis plan?
If you do not have enough information, what questions would you need to ask to obtain the information to run your analysis?
  
--> 
1) Data Collection:
Gather data from various sources, such as student records, course evaluations, and academic performance databases.
Collect information on student demographics (e.g., age, gender, ethnicity), academic history (e.g., GPA, standardized test scores), and course-related variables (e.g., attendance, participation, homework scores).
Take final grade as target variable.

2) Data Cleaning and Preprocessing:
  Check for unique values in all columns. Remove variables which have unique identifiers such as StudentId, Roll Number etc.
Check for missing data in the collected variables and decide on appropriate strategies for handling missing values (e.g., imputation, deletion).
Examine the distribution of numerical variables and identify outliers that may need to be addressed.
Convert categorical variables into dummy variables if necessary to include them in the regression model.

3) Exploratory Data Analysis (EDA):
  Visualize the relationships between predictor variables (e.g., study hours, previous grades) and the target variable (final grade) using scatter plots, histograms, and correlation matrices.
Explore potential multicollinearity among predictor variables to ensure they are not highly correlated with each other, as this could affect the stability and interpretability of the regression coefficients.

4) Model Building:
  Split the dataset into training and testing sets to evaluate the performance of the regression model.
Select appropriate predictor variables based on theoretical considerations, domain knowledge, and statistical significance.
Fit a linear regression model using the selected predictor variables and the final grade as the target variable.
Consider including interaction terms or polynomial terms if there is evidence of nonlinear relationships between predictors and the target variable.

5) Model Evaluation:
  Assess the goodness of fit of the regression model using metrics such as R-squared, adjusted R-squared, and root mean squared error (RMSE).
Examine the normality of residuals and homoscedasticity to ensure that the assumptions of linear regression are met.
Evaluate the performance of the model on the test set to determine its predictive accuracy and generalizability to new data.

6) Interpretation:
  Interpret the coefficients of the regression model to understand the direction and strength of the relationships between predictor variables and the final grade.

Questions to obtain more information:
  1) What kinds of information do we have about students in the dataset?
  2) Is there anything we should worry about regarding the quality of the data or how it was collected?
  3) Are there any other things we need to think about that might affect our results?
  We're asking if there might be other factors we need to consider, like if some students had different opportunities or experiences that could change their grades.
4) Do we know anything about how the class was taught or how students were graded?
We're wondering if there's any extra information about how the class worked or how students were evaluated that could help us understand the grades better.
5) Are there any rules or concerns about privacy or being fair to the students when we use this data?
We need to make sure that we are following the rules and being respectful to the students' privacy when we use their information for our study.


# --------------------------------------------------------------------------------

# Question 9:

# Load necessary libraries
library(tidyverse)
library(dplyr)
library(modeest)  # for the mfv function to find the mode
library(caret)    # For correlation
library(fastDummies)


# Importing data in R
df <- read.csv("D:/Assignments_Depaul/DSC_424_Advance_Data_Analysis/Midterm Exam/home_prices.csv", header = TRUE)
dim(df)
head(df)
View(df)

# Display summary statistics
summary(df)

# Checking the class of the columns
column_types <- sapply(df, class)
print(column_types)

# Count the number of categorical and numerical variables
num_categorical <- sum(column_types == "factor" | column_types == "character")
num_numerical <- sum(column_types == "numeric" | column_types == "integer")

# Print the results
cat("Number of Categorical Variables:", num_categorical, "\n")
cat("Number of Numerical Variables:", num_numerical, "\n")

# Checking number of unique values
unique_counts <- sapply(df, function(x) length(unique(x)))

# Print the number of unique values for each column
print(unique_counts)

# Checking if data has NA values columnwise
na_percentages <- colMeans(is.na(df)) * 100
na_percentages

# Calculate the percentage of rows with NA
percentage_na_rows <- mean(apply(df, 1, function(row) any(is.na(row)))) * 100
print(percentage_na_rows)
# No Missing values present in the data frame.

# ----------------------------------- Target Variable Analysis

# Plotting histogram of target variable

hist(df$price_of_house, main = "Histogram of price_of_house", xlab = "price_of_house", col = "skyblue", border = "black")

price_of_house_skewness <- skewness(df$price_of_house)

cat("Skewness of Sale_Price:", price_of_house_skewness, "\n")

# Finding outliers

# Calculate Z-scores
z_scores <- scale(df$price_of_house)

# Set a threshold (e.g., 3 or -3)
threshold <- 3

# Identify outliers
outliers <- which(abs(z_scores) > threshold)

# Print the indices of outliers
cat("Indices of outliers in Sale_Price:", outliers, "\n")

# Print the values of outliers
cat("Values of outliers in Sale_Price:", df$price_of_house[outliers], "\n")


# Remove rows with outliers
df <- df[-outliers, ]

# Print information about removed rows
cat("Number of rows removed:", length(outliers), "\n")

hist(df$price_of_house, main = "Histogram of price_of_house", xlab = "price_of_house", col = "skyblue", border = "black")

skewness(df$price_of_house)

# Identify numeric and categorical columns
numeric_cols <- sapply(df, is.numeric)
categorical_cols <- sapply(df, function(x) is.factor(x) | is.character(x))

# Create df_numeric and df_categorical
df_numeric <- df[, numeric_cols]
df_categorical <- df[, categorical_cols]

# --------------------------------- Correlation check -------------------------------

# Question 1: 

# Checking Correlation
correlation_matrix <- cor(df_numeric)
correlation_matrix

library(corrplot)
corrplot(correlation_matrix, method = "circle")

# By checking correlation matrix, we can clearly see that all the variables have either less or moderate correlation with each other as well as with target variable.
# Hence, no need to remove any of the variable as no veriable is highly correlated.
# There are some variables with very less correlation values but we will try converting those variables into factors as there might be any non-linear relations between those variables and target variable because they have less number of unique values.

# Calculate VIF scores
library(car)
vif_scores <- vif(lm(formula = df$price_of_house ~ ., data = df_numeric))

# Print VIF scores
print(vif_scores)

# All the variables have vif value less than 10 hence we can say there is no multicollinearity.

# ---------------------------- Converting variables to factors

plot(df$number_of_bedrooms, df$price_of_house, 
     xlab = "number_of_bedrooms", ylab = "price_of_house",
     main = "Scatter Plot: number_of_bedrooms vs price_of_house")

plot(df$Number_of_house_stories, df$price_of_house, 
     xlab = "Number_of_house_stories", ylab = "price_of_house",
     main = "Scatter Plot: Number_of_house_stories vs price_of_house")

df$number_of_bedrooms <- factor(df$number_of_bedrooms)
df$number_of_bathrooms <- factor(df$number_of_bathrooms)
df$Number_of_house_stories <- factor(df$Number_of_house_stories)
df$Number_of_parking_spaces <- factor(df$Number_of_parking_spaces)   

sapply(df, class)

# ----------------------------------- Combining Data ----------------------------------

# Identify numeric and categorical columns again
numeric_cols <- sapply(df, is.numeric)
categorical_cols <- sapply(df, function(x) is.factor(x) | is.character(x))

# Create df_numeric and df_categorical
df_numeric <- df[, numeric_cols]
df_categorical <- df[, categorical_cols]

# Creating dummy variables

New_df <- cbind(df_numeric, df_categorical)

df_combined_dummies <- New_df %>% model.matrix(~ . - 1, data = .) %>%  as.data.frame()
dim(df_combined_dummies)

# ----------------------------------- Splitting Data -------------------------------------------------

# Creating a train/test partition
set.seed(123) 
splitIndex <- createDataPartition(df_combined_dummies$price_of_house, p = 0.8, list = FALSE)
df_train <- df_combined_dummies[splitIndex, ]
df_test <- df_combined_dummies[-splitIndex, ]

dim(df_train)
dim(df_test)

# Question 2:

# Apply linear regression
Initial_model <- lm(price_of_house ~ ., data=df_train)
summary(Initial_model)

# R-squared value is 0.69, indicating that approximately 69% of the variance in house prices is accounted for by the predictor variables in the model.
# Adjusted R-squared value is 0.6734. The adjusted R-squared value adjusts the R-squared value for the number of predictors in the model, providing a more accurate measure of model fit, especially when comparing models with different numbers of predictors.
# The F-statistic tests the overall significance of the regression model by comparing the variance explained by the model to the variance not explained. The low p-value (< 2.2e-16) associated with the F-statistic suggests that the regression model is statistically significant, indicating that at least one of the predictor variables has a non-zero coefficient.
# The table under "Coefficients" provides information about the significance of individual predictor variables. Variables with p-values less than the 0.05 are considered statistically significant.
# Variables with p-values marked with asterisks (***) are highly significant
# The "Estimate" column provides the estimated coefficients (beta coefficients) of the predictor variables. These coefficients represent the change in the dependent variable for a one-unit change in the predictor variable, holding all other variables constant.
# For significant predictor variables, the beta coefficients indicate the direction and magnitude of the relationship between the predictor variable and the dependent variable. Positive coefficients indicate a positive relationship (increase in predictor variable leads to an increase in the dependent variable), while negative coefficients indicate a negative relationship (increase in predictor variable leads to a decrease in the dependent variable).

# We will perform backward elimination model to select significant variables

# Perform backward elimination using stepwise regression
backward_model <- step(Initial_model, direction = "backward")

# Summary of final model after backward elimination
summary(backward_model)

area_of_house: For every one unit increase in the area of the house, the price_of_house is estimated to increase by $230.86, on average.
number_of_bedrooms2: For houses with two bedrooms compared to houses with one bedroom, the price_of_house is estimated to decrease by $294,082.48, on average.
number_of_bathrooms2: For houses with two bathrooms compared to houses with one bathroom, the price_of_house is estimated to increase by $812,316.26, on average.
number_of_bathrooms3: For houses with three bathrooms compared to houses with one bathroom, the price_of_house is estimated to increase by $1,797,366.10, on average.
Number_of_house_stories2: For houses with two stories compared to houses with one story, the price_of_house is estimated to increase by $286,784.87, on average.
Number_of_house_stories3: For houses with three stories compared to houses with one story, the price_of_house is estimated to increase by $654,299.34, on average.
Number_of_house_stories4: For houses with four stories compared to houses with one story, the price_of_house is estimated to increase by $1,593,071.30, on average.
On_mainroadyes: For houses located on a main road compared to those not on a main road, the price_of_house is estimated to increase by $514,626.38, on average.
Has_guestroomyes: For houses with a guest room compared to those without, the price_of_house is estimated to increase by $382,236.24, on average.
Has_basementyes: For houses with a basement compared to those without, the price_of_house is estimated to increase by $303,916.67, on average.
Has_hotwaterheatingyes: For houses with hot water heating compared to those without, the price_of_house is estimated to increase by $1,115,111.00, on average.
Has_airconditioningyes: For houses with air conditioning compared to those without, the price_of_house is estimated to increase by $723,155.01, on average.
Number_of_parking_spaces1: For houses with one parking space compared to those without, the price_of_house is estimated to increase by $315,843.01, on average.
Number_of_parking_spaces2: For houses with two parking spaces compared to those without, the price_of_house is estimated to increase by $557,990.60, on average.
in_preferred_areayes: For houses in a preferred area compared to those not in a preferred area, the price_of_house is estimated to increase by $509,324.57, on average.
is_furnishedunfurnished: For houses that are unfurnished compared to those that are fully furnished, the price_of_house is estimated to decrease by $420,683.22, on average.

# Equation:
price_of_house = 2019040.05 + (230.86 * area_of_house) - (294082.48 * number_of_bedrooms2) + (812316.26 * number_of_bathrooms2) + (1797366.10 * number_of_bathrooms3) + 
  (286784.87 * Number_of_house_stories2) + (654299.34 * Number_of_house_stories3) + (1593071.30 * Number_of_house_stories4) + 
  (514626.38 * On_mainroadyes) + (382236.24 * Has_guestroomyes) + (303916.67 * Has_basementyes) + 
  (1115111.00 * Has_hotwaterheatingyes) + (723155.01 * Has_airconditioningyes) + (315843.01 * Number_of_parking_spaces1) + 
  (557990.60 * Number_of_parking_spaces2) + (509324.57 * in_preferred_areayes) + (-420683.22 * is_furnishedunfurnished)

# ------------------------------------- Lassso Regression --------------------

#Question 3:

# Load the glmnet package
library(glmnet)

# Fit the Lasso regression model
lasso_model <- cv.glmnet(as.matrix(df_train[, -1]), df_train$price_of_house, alpha = 1)

# Print the summary of the Lasso model
print(lasso_model)

# Display optimal lambda value
best_lambda <- lasso_model$lambda.min
print(paste("Optimal lambda:", best_lambda))

# Display coefficients
lasso_coef <- coef(lasso_model, s = best_lambda)
print(lasso_coef)


# The results of the Lasso regression are different from the initial linear regression model. Lasso regression introduces a penalty term that encourages sparsity in the coefficients, leading to some coefficients being exactly zero. This is evident in the output where some coefficients are shown as "." indicating zero.
# Benifit:
# The benefit of using Lasso regression for this research question is that it automatically selects the most important features by shrinking the less important ones to zero.
# Lasso regression made coefficients of variables number_of_bedrooms3 and number_of_bedrooms4 to 0. Hence lasso regression performed variable selection here.
# Disadvantages:
# The cost of using Lasso regression is that it may discard some potentially useful variables, leading to a simpler but less interpretable model. Moreover, the choice of the regularization parameter (lambda) needs to be optimized, which might require cross-validation.

# --------------------------------------------------- EDA ------------------------------------------------------

# Question 4:

colnames(New_df)

# Scatter plot between area_of_house and price_of_house
ggplot(New_df, aes(x = area_of_house, y = price_of_house)) +
  geom_point() +
  labs(x = "Area of House", y = "Price of House") +
  ggtitle("Scatter Plot of Price vs. Area of House")
# By looking at the scatterplot, we can see that there is moderate positive linear relation between the varialbes. Hence the variable area_of_house will be a significant variable while predicting the price.


# Boxplots
ggplot(New_df, aes(x = factor(number_of_bedrooms), y = price_of_house)) +
  geom_boxplot() +
  labs(x = "Number of Bedrooms", y = "Price of House") +
  ggtitle("Boxplot of Price vs. Number of Bedrooms")
# In above plot, we can clearly see that the increase in median value of Number of bedrooms increases the price of the house.
# So that after converting the variable to factor, this variable might be significant for us to predict the price.


ggplot(New_df, aes(x = factor(number_of_bathrooms), y = price_of_house)) +
  geom_boxplot() +
  labs(x = "Number of Bathrooms", y = "Price of House") +
  ggtitle("Boxplot of Price vs. Number of Bathrooms")
# Price increases as number of bathrooms increases. Number of bathrooms are higher as we increase the price of the house.

ggplot(New_df, aes(x = factor(Number_of_house_stories), y = price_of_house)) +
  geom_boxplot() +
  labs(x = "Number of House Stories", y = "Price of House") +
  ggtitle("Boxplot of Price vs. Number of House Stories")
# Price is higher for higher number of house stories. 

ggplot(New_df, aes(x = factor(On_mainroad), y = price_of_house)) +
  geom_boxplot() +
  labs(x = "On Main Road", y = "Price of House") +
  ggtitle("Boxplot of Price vs. On Main Road")
# The price of house is high if a house is on main road. If a house is not on main road, price of the house is less.

ggplot(New_df, aes(x = factor(Has_guestroom), y = price_of_house)) +
  geom_boxplot() +
  labs(x = "Has Guestroom", y = "Price of House") +
  ggtitle("Boxplot of Price vs. Has Guestroom")
# If a house has guestroom, then price is high as compared to house without guestroom.

ggplot(New_df, aes(x = factor(Has_basement), y = price_of_house)) +
  geom_boxplot() +
  labs(x = "Has Basement", y = "Price of House") +
  ggtitle("Boxplot of Price vs. Has Basement")
# Price of house is greater if a house has basement.

ggplot(New_df, aes(x = factor(Has_hotwaterheating), y = price_of_house)) +
  geom_boxplot() +
  labs(x = "Has Hot Water Heating", y = "Price of House") +
  ggtitle("Boxplot of Price vs. Has Hot Water Heating")
# If a house has hot water heating system, then the price is heigher as compared to house without water heating.

ggplot(New_df, aes(x = factor(Has_airconditioning), y = price_of_house)) +
  geom_boxplot() +
  labs(x = "Has Air Conditioning", y = "Price of House") +
  ggtitle("Boxplot of Price vs. Has Air Conditioning")
# Price is high for houses with Air Conditioning. The price is lower for houses without air conditioning.

ggplot(New_df, aes(x = factor(Number_of_parking_spaces), y = price_of_house)) +
  geom_boxplot() +
  labs(x = "Number of Parking Spaces", y = "Price of House") +
  ggtitle("Boxplot of Price vs. Number of Parking Spaces")
# Prices increases as we increase the number of parking spaces in a house.

ggplot(New_df, aes(x = factor(in_preferred_area), y = price_of_house)) +
  geom_boxplot() +
  labs(x = "In Preferred Area", y = "Price of House") +
  ggtitle("Boxplot of Price vs. In Preferred Area")
# If a house is in preferred area, then the house price is high.

# -------------------------------------------- Now we will test our backward selection model ----------------

# Making predictions on test data
predictions <- predict(backward_model, newdata = df_test)
dim(df_test)

# Calculate Mean Squared Error (MSE)
mse_initial <- mean((df_test$price_of_house - predictions)^2)
cat("Mean Squared Error (MSE):", mse_initial, "\n")

# Calculate Mean Absolute Error (MAE)
mae_initial <- mean(abs(df_test$price_of_house - predictions))
cat("Mean Absolute Error (MAE):", mae_initial, "\n")

# Residual Analysis

residuals <- rstudent(backward_model)
predicted_values <- predict(backward_model)

plot(predicted_values, residuals, main="Studentized Residuals vs. Predicted Values",
     xlab="Predicted Values", ylab="Studentized Residuals", col="blue", pch=16)
abline(h=0, col="red")

# With the help of residuals plot, we can see that residuals have high value for high predicted values. Hence this is a moderate model. We need to imporve this.
# As per my understanding, this is due to the less number of observations and predictors. We need more data to imporve this model further.

# Create a normal probability plot

qqnorm(rstandard(Initial_model), main="Normal Q-Q Plot")
qqline(rstandard(Initial_model), col="red")

cooksd <- cooks.distance(Initial_model)
# Find indices of influential points with Cook's distance > 1
influential_indices <- which(cooksd > 1)

library(car)
influenceIndexPlot(Initial_model)

# We have less number of influential points. 

residuals_df <- data.frame(
  Actual = df_test$price_of_house,
  Predicted = predictions,
  Residuals = df_test$price_of_house - predictions
)

# Plot histogram or density plot of residuals
ggplot(residuals_df, aes(x = Residuals)) +
  geom_histogram(binwidth = 100000, fill = "blue", color = "white", alpha = 0.7) +
  labs(title = "Distribution of Residuals", x = "Residuals", y = "Frequency")

skewness(residuals_df$Residuals)
# The residual plot looks like normally distributed. Also skewness is 0.5192377 which is under acceptable range.


# -------------------------------------------------------------------------------------------------------

# Question 10: 

#Libraries
library(Hmisc) #Describe Function
library(psych) #Multiple Functions for Statistics and Multivariate Analysis
library(GGally) #ggpairs Function
library(ggplot2) #ggplot2 Functions
library(vioplot) #Violin Plot Function
library(corrplot) #Plot Correlations
library(REdaS) #Bartlett's Test of Sphericity
library(psych) #PCA/FA functions
library(factoextra) #PCA Visualizations
library("FactoMineR") #PCA functions
library(ade4) #PCA Visualizations

# Importing data in R
data <- read.csv("D:/Assignments_Depaul/DSC_424_Advance_Data_Analysis/Midterm Exam/16PF.csv", header = TRUE)
dim(data)
View(data)

# Check NA For All Variables
sum(is.na(data))

library(dplyr)
# Convert 0 to NA as told in the problem statement
data <- data %>%
  mutate_all(~ifelse(. == 0, NA, .))

# Check NA For All Variables
sum(is.na(data))

na_percentages <- colMeans(is.na(data)) * 100
na_percentages

# Calculate the percentage of rows with NA
percentage_na_rows <- mean(apply(data, 1, function(row) any(is.na(row)))) * 100
print(percentage_na_rows)

# Creating a function to impute NA values
imputeNA <- function(data) {
  for (col in names(data)) {
    if (is.numeric(data[[col]])) {
      # Calculate rounded mean
      mean_val <- round(mean(data[[col]], na.rm = TRUE))
      # Impute NA with rounded mean for numeric variables
      data[[col]][is.na(data[[col]])] <- mean_val
    } else if (is.factor(data[[col]]) || is.character(data[[col]])) {
      # Calculate mode
      mode_val <- as.character(sort(table(data[[col]]), decreasing = TRUE)[1])
      # Impute NA with mode for categorical or factor variables
      data[[col]][is.na(data[[col]])] <- mode_val
    }
    # If neither numeric nor categorical, do nothing
  }
  return(data)
}


data<-imputeNA(data)

unique_counts <- sapply(data, function(x) length(unique(x)))
unique_counts

# Calculate the percentage of rows with NA after imputaion
percentage_na_rows_1 <- mean(apply(df, 1, function(row) any(is.na(row)))) * 100
print(percentage_na_rows_1)

# Checking the corrplot matrix
cor_matrix <- cor(data)

library(caret)
highly_correlated_vars <- findCorrelation(cor_matrix, cutoff = 0.75)  
colnames(data[highly_correlated_vars])

# Removing highly correlated columns from data
data <- data[, !colnames(data) %in% c("H3", "J10")]
dim(data)

# Test KMO Sampling Adequancy
library(psych)
KMO(data)
# The Kaiser-Meyer-Olkin (KMO) measure evaluates the adequacy of data for factor analysis, with an overall MSA of 0.97 indicating high correlation among variables. Each item's MSA, ideally close to 1, reflects its correlation strength with other variables, suggesting suitability for factor analysis.

# Test Bartlett's test of Sphericity
library(REdaS)
bart_spher(data)
# Bartlett's Test of Sphericity checks if variables in your data are related or if they act independently. With a p-value less than 0.05 (2.22e-16), it means there are significant relationships between the variables, suggesting they are not completely independent.

#Parallel Analysis (Horn's parallel analysis)
comp <- fa.parallel(data)
comp
# Parallel analysis suggests that the number of factors =  26  and the number of components =  21 

# ---------------------- PCA_Plot functions

PCA_Plot = function(pcaData)
{
  library(ggplot2)
  
  theta = seq(0,2*pi,length.out = 100)
  circle = data.frame(x = cos(theta), y = sin(theta))
  p = ggplot(circle,aes(x,y)) + geom_path()
  
  loadings = data.frame(pcaData$rotation, .names = row.names(pcaData$rotation))
  p + geom_text(data=loadings, mapping=aes(x = PC1, y = PC2, label = .names, colour = .names, fontface="bold")) +
    coord_fixed(ratio=1) + labs(x = "PC1", y = "PC2")
}

PCA_Plot_Secondary = function(pcaData)
{
  library(ggplot2)
  
  theta = seq(0,2*pi,length.out = 100)
  circle = data.frame(x = cos(theta), y = sin(theta))
  p = ggplot(circle,aes(x,y)) + geom_path()
  
  loadings = data.frame(pcaData$rotation, .names = row.names(pcaData$rotation))
  p + geom_text(data=loadings, mapping=aes(x = PC3, y = PC4, label = .names, colour = .names, fontface="bold")) +
    coord_fixed(ratio=1) + labs(x = "PC3", y = "PC4")
}

PCA_Plot_Psyc = function(pcaData)
{
  library(ggplot2)
  
  theta = seq(0,2*pi,length.out = 100)
  circle = data.frame(x = cos(theta), y = sin(theta))
  p = ggplot(circle,aes(x,y)) + geom_path()
  
  loadings = as.data.frame(unclass(pcaData$loadings))
  s = rep(0, ncol(loadings))
  for (i in 1:ncol(loadings))
  {
    s[i] = 0
    for (j in 1:nrow(loadings))
      s[i] = s[i] + loadings[j, i]^2
    s[i] = sqrt(s[i])
  }
  
  for (i in 1:ncol(loadings))
    loadings[, i] = loadings[, i] / s[i]
  
  loadings$.names = row.names(loadings)
  
  p + geom_text(data=loadings, mapping=aes(x = PC1, y = PC2, label = .names, colour = .names, fontface="bold")) +
    coord_fixed(ratio=1) + labs(x = "PC1", y = "PC2")
}

PCA_Plot_Psyc_Secondary = function(pcaData)
{
  library(ggplot2)
  
  theta = seq(0,2*pi,length.out = 100)
  circle = data.frame(x = cos(theta), y = sin(theta))
  p = ggplot(circle,aes(x,y)) + geom_path()
  
  loadings = as.data.frame(unclass(pcaData$loadings))
  s = rep(0, ncol(loadings))
  for (i in 1:ncol(loadings))
  {
    s[i] = 0
    for (j in 1:nrow(loadings))
      s[i] = s[i] + loadings[j, i]^2
    s[i] = sqrt(s[i])
  }
  
  for (i in 1:ncol(loadings))
    loadings[, i] = loadings[, i] / s[i]
  
  loadings$.names = row.names(loadings)
  
  print(loadings)
  p + geom_text(data=loadings, mapping=aes(x = PC3, y = PC4, label = .names, colour = .names, fontface="bold")) +
    coord_fixed(ratio=1) + labs(x = "PC3", y = "PC4")
}


# ----------------------------------- Create PCA 

PCA = prcomp(data, center = T, scale = T)

# Checking the scree plot
plot(PCA, main="Scree plot", xlab="PC")
abline(1,0)

#Check PCA visualizations
PCA_Plot(PCA) #PCA_plot1
PCA_Plot_Secondary(PCA) #PCA_Plot2
biplot(PCA) #Biplot

# Extract the cumulative proportion of variance explained
cumulative_variance <- cumsum(PCA$sdev^2) / sum(PCA$sdev^2)

# Find the number of components needed to account for 80% of the variance
num_components <- which(cumulative_variance >= 0.8)[1]
num_components

# 85 components are needed to account for 80% of the variance in the data. The number of components is determined by identifying the smallest number of principal components where the cumulative proportion of variance explained by those components reaches or exceeds 80%. This is calculated by summing up the variances explained by each component until the cumulative proportion exceeds the specified threshold (in this case, 80%). The which function in R is then used to find the index of the first component that meets this criterion.

# ------------------------------------------

# Question 2

# Eigenvalue method
eigenvalues <- PCA$sdev^2
num_components_eigenvalue <- sum(eigenvalues > 1)
num_components_eigenvalue

# With the help of eigen values, we will take 25 components which have eigen values > 1.

# Knee of the scree plot method
scree_values <- PCA$sdev^2
variance_explained <- scree_values / sum(scree_values)
num_components_scree_0.05 <- which.max(diff(variance_explained) < 0.05) + 1  
num_components_scree_0.01 <- which.max(diff(variance_explained) < 0.05) + 1 
num_components_scree_0.05
num_components_scree_0.01

# If we are using the knee of the scree plot, we can choose 2 components only. But those components explains around 19% of the varience only.

#----------------------------------------------

# Question 3:

# Get the loadings of the top 10 variables for the first component
top_loadings <- abs(PCA$rotation[, 1])  # Absolute values of loadings for first component
top_loadings <- sort(top_loadings, decreasing = TRUE)[1:10]  # Top 10 loadings
top_variables <- names(top_loadings)
top_variables

# Plot the top variables
barplot(top_loadings, names.arg = top_variables, xlab = "Variable", ylab = "Loading")

# Question 3: i) 
# We will choose the eigen value method to choose the number of components.
# With the eigenvalue method, 25 components are chosen which have eigenvalues greater than 1.
# We choose this method because variation explained method is giving us 85 components which are explaining 80% of the variation in the data and knee of scree plot is giving 2 variables with 19% of the variation which is too less.


# Extract the loadings of each principal component
loadings <- PCA$rotation

# Define a function to interpret each component
interpret_component <- function(component_number, top_n = 5) {         # Taking top 5 variables

  component_loadings <- loadings[, component_number]

  sorted_loadings <- sort(abs(component_loadings), decreasing = TRUE)
  
  top_variable_names <- names(sorted_loadings)[1:top_n]
  
  interpretation <- paste("Component", component_number, "is primarily influenced by the following variables:")
  for (variable_name in top_variable_names) {
    interpretation <- paste(interpretation, variable_name, sep = " ")
  }
  
  return(interpretation)
}

# Interpret the first 25 components
for (i in 1:25) {
  cat(interpret_component(i), "\n\n")
}

# Question 3: ii)

PC1: This component explains approximately 12.89% of the total variance in the dataset, capturing a substantial portion of the overall variation.
PC2: Accounting for around 19.02% of the variance, PC2 contributes significantly to understanding additional patterns not captured by PC1.
PC3: With approximately 24.63% of the variance explained, PC3 further expands on the variability present in the data, potentially capturing more nuanced relationships.
PC4: Explaining about 29.08% of the variance, PC4 continues to contribute significantly to understanding the data's structure.
PC5: Capturing around 32.72% of the variance, PC5 adds to the understanding of unique patterns and relationships in the data.
PC6: Explaining about 35.46% of the variance, PC6 contributes notably to the overall variability captured by the model.
PC7: With approximately 37.29% of the variance explained, PC7 continues to enrich our understanding of the data's structure.
PC8: Accounting for around 38.93% of the variance, PC8 adds further insights into the variability present in the dataset.
PC9: Explaining about 40.33% of the variance, PC9 contributes significantly to understanding additional patterns beyond the previous components.
PC10: Capturing approximately 41.54% of the variance, PC10 continues to provide valuable information about the data's structure.
PC11 captures additional unique patterns in the data, explaining approximately 42.70% of the total variance beyond what the previous components have accounted for.
PC12 further contributes to explaining the variability in the dataset, accounting for approximately 44.80% of the total variance.
PC13 continues to capture distinct patterns, explaining approximately 45.79% of the total variance.
PC14 adds to the understanding of the dataset by explaining approximately 46.71% of the total variance.
PC15 provides insight into additional underlying structures, explaining approximately 47.57% of the total variance.
PC16 uncovers further patterns in the data, explaining approximately 48.38% of the total variance.
PC17 continues the trend of revealing unique aspects, explaining approximately 49.27% of the total variance.
PC18 contributes to understanding the dataset by explaining approximately 49.97% of the total variance.
PC19 captures additional variation in the data, explaining approximately 50.64% of the total variance.
PC20 provides further insights into the underlying structure, explaining approximately 51.34% of the total variance.
PC21 continues to reveal unique patterns, explaining approximately 51.98% of the total variance.
PC22 adds to the understanding of the dataset by explaining approximately 52.64% of the total variance.
PC23 uncovers additional variation in the data, explaining approximately 53.27% of the total variance.
PC24 contributes to understanding the dataset by explaining approximately 53.87% of the total variance.
PC25 provides further insights into the underlying structure, explaining approximately 54.39% of the total variance.
'

# Calculating equation of first component
loadings_first_component <- PCA$rotation[1,]
variable_names <- colnames(data)
equation <- "PC1 = "

for (i in seq_along(loadings_first_component)) {
  # Append each term to the equation string
  equation <- paste(equation, paste(loadings_first_component[i], "*", variable_names[i]), sep = " + ")
}

print(equation)
# PC1 = 0.107688837512609 * A1 - 0.000607634358017455 * A2 + 0.0717719266179847 * A3 - 0.104288909915405 * A4 + 0.0361707877257911 * A5 - 0.0377196528950324 * A6 + 0.0108627405502496 * A7 - 0.0949412631864418 * A8 + 0.0742239562877308 * A9 - 0.202099188492801 * A10 + 0.134190824071809 * B1 - 0.0440628815146651 * B2 + 0.0583842488242584 * B3 - 0.0125127655564609 * B4 + 0.0258549766627675 * B5 - 0.082774611595253 * B6 + 0.0197545936886563 * B7 - 0.06615985238417 * B8 + 0.090148382094934 * B9 - 0.0831653291469757 * B10 + 0.0341752619477459 * B11 + 0.027465466150087 * B12 - 0.115583127702119 * B13 + 0.0369037672722159 * C1 - 0.0186301481933703 * C2 + 0.0458493302425082 * C3 - 0.0536567313906488 * C4 + 0.00626231638061165 * C5 - 0.138474511065638 * C6 + 0.00813740484688702 * C7 - 0.0293024187032727 * C8 + 0.0132597698718794 * C9 - 0.0206882834086204 * C10 + 0.0253930781344029 * D1 - 0.0266849240677188 * D2 + 0.0299298775960761 * D3 - 0.0330785191278043 * D4 + 0.0232053360060502 * D5 - 0.109001417272977 * D6 - 0.00744055306777567 * D7 + 0.00526716342822014 * D8 + 0.167566780936231 * D9 - 0.00499994108811575 * D10 - 0.0345310312771531 * E1 + 0.0436322197732453 * E2 + 0.0344586643534838 * E3 - 0.07464926431246 * E4 - 0.0148308699522505 * E5 + 0.0173304210981651 * E6 + 0.0623276110200467 * E7 - 0.0265983223814058 * E8 - 0.0325463689286593 * E9 - 0.0102225363394566 * E10 - 0.0430668943595973 * F1 + 0.046650786848225 * F2 + 0.0363525902207215 * F3 + 0.0129763537649023 * F4 + 0.0593034776859585 * F5 - 0.00852878187425003 * F6 - 0.0511123648951801 * F7 - 0.0142356438181979 * F8 + 0.0122071665311602 * F9 + 0.0889436155965487 * F10 + 0.0495503355154718 * G1 + 0.0205705001045581 * G2 - 0.000252522322346653 * G3 - 0.0473884919864824 * G4 - 0.04471103366004 * G5 - 0.00713374521976899 * G6 - 0.102983510101151 * G7 + 0.00905197854065297 * G8 + 0.00351666000748918 * G9 + 0.132922024482152 * G10 + 7.87871979465952e-05 * H1 - 0.0452895221633688 * H2 - 0.121535385651749 * H4 - 0.0592903955366528 * H5 - 0.0183661785037103 * H6 - 0.114483601182519 * H7 + 0.044247286997945 * H8 + 0.0177113885615819 * H9 + 0.0479810294533258 * H10 + 0.0207156233961283 * I1 + 0.0510543636023547 * I2 - 0.00261263664171243 * I3 - 0.00338225368848512 * I4 - 0.126094042095063 * I5 - 0.0018658179770743 * I6 + 0.029368333604155 * I7 + 0.104063256303854 * I8 + 0.0333547434745203 * I9 - 0.0362380143811021 * I10 - 0.0221775207886494 * J1 - 0.0443165118262443 * J2 - 0.0276167022101894 * J3 - 0.0304943957753827 * J4 - 0.0260631444243015 * J5 - 0.0265074122823339 * J6 + 0.0992938199235987 * J7 + 0.0137499795398283 * J8 + 0.00304779774376655 * J9 + 0.00616726376684408 * K1 - 0.0118389335848942 * K2 - 0.0671177757988993 * K3 - 0.0564937965780474 * K4 - 0.167245148214775 * K5 + 0.0412848840549026 * K6 + 0.0320885474808324 * K7 + 0.0225769257023241 * K8 + 0.0255680441062135 * K9 + 0.0680148160697296 * K10 + 0.111627201580665 * L1 - 0.00926535505586133 * L2 - 0.0159699089755806 * L3 - 0.0831364779021479 * L4 - 0.0507479751767364 * L5 - 0.115335683785823 * L6 + 0.143979335499705 * L7 + 0.201953829364046 * L8 + 0.0182134565729764 * L9 + 0.177001867941051 * L10 - 0.156978626258852 * M1 - 0.314903929746294 * M2 - 0.214936911951467 * M3 + 0.0385273157715941 * M4 + 0.201232969024521 * M5 + 0.146675990022069 * M6 - 0.0375304179677273 * M7 - 0.0833523469773029 * M8 - 0.0222794261571607 * M9 - 0.137409408642863 * M10 - 0.0573389571745327 * N1 + 0.0105016761904649 * N2 + 0.0431237199273686 * N3 + 0.252237272241084 * N4 + 0.0477283244505034 * N5 - 0.0759909752701043 * N6 - 0.122398729741589 * N7 - 0.0958042382841144 * N8 + 0.00640965310133097 * N9 + 0.126458533601053 * N10 - 0.118256919990697 * O1 - 0.0990494820415183 * O2 - 0.0161346838278886 * O3 + 0.0918516672968629 * O4 + 0.0274410180270775 * O5 - 0.0219994010334594 * O6 - 0.0507228628419817 * O7 + 0.0334856025596856 * O8 + 0.0190071188519293 * O9 - 0.115365905984597 * O10 + 0.0369193513528551 * P1 - 0.0281681480766451 * P2 + 0.00416515271667506 * P3 - 0.000921787008894993 * P4 + 0.0202823279788101 * P5 - 0.00268909563624996 * P6 + 0.00405948203126332 * P7 - 0.0128736833495133 * P8 + 0.00101108701966617 * P9 + 0.112262435240244 * P10


# Question 4:

# Extract component scores for first 25 elements
component_scores <- PCA$x[, 1:25] 

# Get the five-number summary
summary_component_scores <- apply(component_scores, 2, summary)
summary_component_scores

# The five-number summary of the component scores provides insights into the distribution of scores for each component. 
# If the range between the minimum and maximum scores is wide, it indicates a significant variation in the corresponding personality trait among the respondents. On the other hand, if the interquartile range (IQR) between Q1 and Q3 is small, it suggests that most respondents have similar scores for that particular trait.
# The median provides information about the central tendency of the scores, while the quartiles give insights into the spread of scores around the median.
# PC1, PC2, PC4, PC7, PC10, PC11, PC14, PC16, PC17, PC19, PC21, PC23, and PC25 have relatively wide score distributions based on the large differences between their minimum and maximum scores and their IQRs.
# PC6, PC8, PC9, PC12, PC13, PC15, PC18, PC20, and PC22 have relatively similar score distributions compared to the other components due to smaller differences between their minimum and maximum scores and their IQRs.

library(stats)

# Perform factor analysis
factor_analysis_result <- factanal(data, factors = 25) 

print(factor_analysis)

# Extracted cumulative variance for the first 25 factors
cumulative_variance_factor_analysis <- c(0.066, 0.110, 0.146, 0.175, 0.202, 0.230, 0.256, 0.281, 0.300, 0.319,
                                         0.337, 0.354, 0.368, 0.379, 0.391, 0.402, 0.411, 0.420, 0.427, 0.433,
                                         0.438, 0.443, 0.448, 0.451, 0.455)

# PCA loadings for first 25 selected components
cumulative_variance_first_25 <- cumulative_variance[1:25]

cumulative_variance_factor_analysis
cumulative_variance_first_25

# Plotting
plot(1:25, cumulative_variance_factor_analysis, type="b", col="blue", pch=16,
     xlab="Number of Factors", ylab="Cumulative Variance Explained",
     main="Comparison of Cumulative Variance Explained")
lines(1:25, cumulative_variance_first_25, type="b", col="red", pch=16)
legend("bottomright", legend=c("Factor Analysis", "PCA"), col=c("blue", "red"), lty=1:1, cex=0.8)

# Extract loadings from PCA
loadings_pca <- PCA$rotation[, 1:25]
head(loadings_pca)

# Extract loadings from Factor Analysis
loadings_FA <- loadings(factor_analysis_result)
head(loadings_FA)

# The varience explained by factor analysis is 0.455 and 0.5390109 for the first 25 components in PCA.
# By looking at the above plot, we can see the variance explained by PCA is having higher value than varience explained by factor analysis.
# The loading from factor analysis are having less values than loadings in PCA. 
# PCA loadings are optimized to maximize variance along the extracted components. 
# Factor Analysis explicitly models underlying latent constructs or factors, and the loadings represent the relationships between the observed variables and these factors.


# Calculate absolute differences between PCA and FA loadings
loadings_diff <- abs(loadings_pca - loadings_FA)

# Identify variables with the largest differences
max_diff <- apply(loadings_diff, 1, max, na.rm = TRUE)

# Identify variables with the largest differences
max_diff <- apply(loadings_diff, 1, max, na.rm = TRUE)
print(max_diff)
# Above values are the difference between PCA loadings and Factor analysis loadings

max_diff_var <- names(which(max_diff == max(max_diff, na.rm = TRUE)))

# Display the variables with the largest differences
for (var in max_diff_var) {
  max_diff_value <- max_diff[var]
  cat("Variable", var, "has the largest difference of", max_diff_value, "between PCA and FA.\n")
}







